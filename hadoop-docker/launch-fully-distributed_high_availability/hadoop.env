# Variables para los archivos de configuracion de hadoop
#
# Estas variables son utilizadas por el archivo start-hadoop.sh para inicializar
# los archivos core-site.xml, hdfs-site.xml, mapred-site.xml y yarn-site.xml
# Los regenera cada vez que se ejecuta el contenedor.
#
# Estructura de las variables
# {prefijo-archivo-hadoop}_{nombre-variable-hadoop}={valor-variable}
# {prefijo-archivo-hadoop} >>> ( CORE_SITE | HDFS_SITE | MAPRED_SITE | YARN_SITE )
# {nombre-variable-hadoop} >>> mismo nombre que la variable de hadoop pero hay que reemplazar ciertos carateres.
# Caracteres reemplazo >>> (":" > "_____" | "_" > "____" | "-" > "___"  | "@" > "__" | "." > "_" )
#
# Ejemplo: 
#       dfs.replication >>> HDFS_SITE_dfs_replication
#       yarn.nodemanager.aux-services >>> YARN_SITE_yarn_nodemanager_aux___services

# Global
HADOOP_DATA=/hadoop-data

# core-site.xml
CORE_SITE_fs_defaultFS=hdfs://mycluster
CORE_SITE_fs_default_name=hdfs://mycluster
CORE_SITE_hadoop_tmp_dir=/tmp/hadoop-tmp
CORE_SITE_hadoop_proxyuser_root_hosts=*
CORE_SITE_hadoop_proxyuser_root_groups=*

CORE_SITE_ha_zookeeper_quorum=zoo-1:2181,zoo-2:2181,zoo-3:2181

# hdfs-site.xml
HDFS_SITE_dfs_replication=2
HDFS_SITE_dfs_name_dir=file:///hadoop-data/hdfs/namenode
HDFS_SITE_dfs_data_dir=file:///hadoop-data/hdfs/datanode
HDFS_SITE_dfs_namenode_checkpoint_dir=file:///hadoop-data/hdfs/namesecondary
HDFS_SITE_heartbeat_recheck_interval=15
HDFS_SITE_dfs_block_size=268435456

HDFS_SITE_dfs_nameservices=mycluster
HDFS_SITE_dfs_ha_namenodes_mycluster=nn1,nn2,nn3
HDFS_SITE_dfs_namenode_rpc___address_mycluster_nn1=hadoop-master-1:9000
HDFS_SITE_dfs_namenode_rpc___address_mycluster_nn2=hadoop-master-2:9000
HDFS_SITE_dfs_namenode_rpc___address_mycluster_nn3=hadoop-master-3:9000
HDFS_SITE_dfs_namenode_http___address_mycluster_nn1=hadoop-master-1:9870
HDFS_SITE_dfs_namenode_http___address_mycluster_nn2=hadoop-master-2:9870
HDFS_SITE_dfs_namenode_http___address_mycluster_nn3=hadoop-master-3:9870
HDFS_SITE_dfs_namenode_shared_edits_dir=qjournal://journalnode-1:8485;journalnode-2:8485;journalnode-3:8485/mycluster
HDFS_SITE_dfs_client_failover_proxy_provider_mycluster=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
HDFS_SITE_dfs_ha_fencing_methods=shell(/bin/true)
HDFS_SITE_dfs_ha_fencing_ssh_private___key___files=/root/.ssh/id_rsa
HDFS_SITE_dfs_ha_fencing_ssh_connect___timeout=30000
HDFS_SITE_dfs_ha_automatic___failover_enabled=true
HDFS_SITE_dfs_journalnode_edits_dir=///hadoop-data/

# mapred-site.xml
MAPRED_SITE_mapreduce_framework_name=yarn
MAPRED_SITE_yarn_app_mapreduce_am_env=HADOOP_MAPRED_HOME=$HADOOP_HOME
MAPRED_SITE_mapreduce_map_env=HADOOP_MAPRED_HOME=$HADOOP_HOME
MAPRED_SITE_mapreduce_reduce_env=HADOOP_MAPRED_HOME=$HADOOP_HOME

# yarn-site.xml
YARN_SITE_yarn_nodemanager_aux___services=mapreduce_shuffle
YARN_SITE_yarn_nodemanager_aux___services_mapreduce____shuffle_class=org.apache.hadoop.mapred.ShuffleHandler
YARN_SITE_yarn_nodemanager_local___dirs=file:///hadoop-data/yarn
YARN_SITE_yarn_resourcemanager_hostname=hadoop-resourcemanager-1